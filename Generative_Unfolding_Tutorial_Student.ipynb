{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skipperglume/AI_suggested_scripts/blob/master/Generative_Unfolding_Tutorial_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gvkeefrgulh9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "id": "2MAI7j8dZXhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4badf53-7359-45ec-ce03-3732441c109b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->torchdiffeq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdiffeq\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchdiffeq-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchdiffeq import odeint"
      ],
      "metadata": {
        "id": "6qMBHWcAZczy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "8tkGxx42uuSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebbbc2f-d0da-4da2-c34b-5237491d5d7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interesting papers\n",
        "\n",
        "- An unfolding method based on conditional invertible neural networks (cINN) using iterative training  <br> *Mathias Backes,  Anja Butter, Monica Dunford and Bogdan Malaescu* <br> https://arxiv.org/pdf/2212.08674\n",
        "\n",
        "- The landscape of unfolding with machine learning <br> *Huetsch et al.* <br> https://arxiv.org/pdf/2404.18807\n",
        "\n",
        "- Flow Matching for Generative Modeling  <br> *Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le* <br>\n",
        "https://arxiv.org/pdf/2210.02747.pdf\n",
        "\n",
        "\n",
        "We are going to work with a dataset from https://zenodo.org/records/3548091. In particular, we will be looking into three jet-substructure observables of the leading jet, the jet width, the soft-drop mass and the $N_{12}$-subjetiness ratio. The goal is to unfold detector effects and infer $p_\\text{data} (x_\\text{reco}) \\rightarrow p_\\text{unfold} (x_\\text{part})$ of the 3-dimensional phase space.\n",
        "\n"
      ],
      "metadata": {
        "id": "aUeh3nSwu6LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "PaRdLNmwRdSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.dropbox.com/scl/fo/88muusw0kecnxv2644ve6/AG8Ov6PSx7UfxPk328qoP-w?rlkey=stexzrk8t0f0almhrkq32ey3l&st=edyovawd&dl=1\"\n",
        "!unzip \"AG8Ov6PSx7UfxPk328qoP-w?rlkey=stexzrk8t0f0almhrkq32ey3l&st=edyovawd&dl=1\""
      ],
      "metadata": {
        "id": "8zoHtKy2O-78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca39e0f0-ab7a-4d4f-9162-4b4050ccfc87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-30 14:14:58--  https://www.dropbox.com/scl/fo/88muusw0kecnxv2644ve6/AG8Ov6PSx7UfxPk328qoP-w?rlkey=stexzrk8t0f0almhrkq32ey3l&st=edyovawd&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1585ad8635b89405e139e0d89e.dl.dropboxusercontent.com/zip_download_get/CP_qi2_w4x9tlPO42r4aYGNCmvEbliNba62OxX-wPVU8UnrSurJ5WW_A5WdhI_Qv9Fu75Mjale-z1w6rozmRsRJCGIrJO2sQbmNsYxKJwWfCoA# [following]\n",
            "--2025-06-30 14:14:59--  https://uc1585ad8635b89405e139e0d89e.dl.dropboxusercontent.com/zip_download_get/CP_qi2_w4x9tlPO42r4aYGNCmvEbliNba62OxX-wPVU8UnrSurJ5WW_A5WdhI_Qv9Fu75Mjale-z1w6rozmRsRJCGIrJO2sQbmNsYxKJwWfCoA\n",
            "Resolving uc1585ad8635b89405e139e0d89e.dl.dropboxusercontent.com (uc1585ad8635b89405e139e0d89e.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc1585ad8635b89405e139e0d89e.dl.dropboxusercontent.com (uc1585ad8635b89405e139e0d89e.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96001166 (92M) [application/zip]\n",
            "Saving to: ‘AG8Ov6PSx7UfxPk328qoP-w?rlkey=stexzrk8t0f0almhrkq32ey3l&st=edyovawd&dl=1’\n",
            "\n",
            "AG8Ov6PSx7UfxPk328q 100%[===================>]  91.55M  46.2MB/s    in 2.0s    \n",
            "\n",
            "2025-06-30 14:15:01 (46.2 MB/s) - ‘AG8Ov6PSx7UfxPk328qoP-w?rlkey=stexzrk8t0f0almhrkq32ey3l&st=edyovawd&dl=1’ saved [96001166/96001166]\n",
            "\n",
            "Archive:  AG8Ov6PSx7UfxPk328qoP-w?rlkey=stexzrk8t0f0almhrkq32ey3l&st=edyovawd&dl=1\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: reco_MC.npy             \n",
            " extracting: part_MC.npy             \n",
            " extracting: reco_data.npy           \n",
            " extracting: part_data.npy           \n",
            "\n",
            "1 archive had fatal errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reco_sim = torch.tensor(np.load(\"reco_MC.npy\"), dtype=torch.float)\n",
        "reco_data = torch.tensor(np.load(\"reco_data.npy\"), dtype=torch.float)\n",
        "part_sim = torch.tensor(np.load(\"part_MC.npy\"),  dtype=torch.float)\n",
        "part_data = torch.tensor(np.load(\"part_data.npy\"),  dtype=torch.float)"
      ],
      "metadata": {
        "id": "EuMdEIPHPCK6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build training and test datasets if needed"
      ],
      "metadata": {
        "id": "s9CvFJDjU491"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reco_sim_train, reco_sim_test = reco_sim[:750000], reco_sim[750000:]\n",
        "reco_data_train, reco_data_test = reco_data[:750000], reco_data[750000:]\n",
        "part_sim_train, part_sim_test = part_sim[:750000], part_sim[750000:]\n",
        "part_data_test = part_data[750000:]"
      ],
      "metadata": {
        "id": "jRiZoUOoU_8H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define some preprocessing functions to apply to the data."
      ],
      "metadata": {
        "id": "XRhJYIfTVJpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.tensor([ 0.1411, -6.9493,  0.6731])\n",
        "std = torch.tensor([0.0960, 2.2009, 0.2031])"
      ],
      "metadata": {
        "id": "D_QkW3xrPJzS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data, mean, std, device=\"cpu\", reverse=False):\n",
        "    \"\"\"\n",
        "    Standardizes or un-standardizes the data.\n",
        "\n",
        "    Args:\n",
        "        data (Tensor): The input data tensor.\n",
        "        mean (Tensor or float): The mean used for standardization.\n",
        "        std (Tensor or float): The standard deviation used for standardization.\n",
        "        device (str or torch.device): The device to move the tensor to ('cpu' or 'cuda').\n",
        "        reverse (bool): If True, reverses the standardization (un-normalize).\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Processed data tensor.\n",
        "    \"\"\"\n",
        "    if not reverse:\n",
        "        data = (data - mean) / std\n",
        "        return data.to(device)\n",
        "    else:\n",
        "        return (data.cpu() * std) + mean"
      ],
      "metadata": {
        "id": "TeEh8NOWPPPa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone and preprocess the data to standardize input features with reco-level MC mean and standard deviation.\n",
        "\n",
        "reco_sim_proc_train = preprocess(reco_sim_train.clone(), mean, std, device=device)\n",
        "reco_sim_proc_test = preprocess(reco_sim_test.clone(), mean, std, device=device)\n",
        "\n",
        "reco_data_proc_train = preprocess(reco_data_train.clone(), mean, std, device=device)\n",
        "reco_data_proc_test = preprocess(reco_data_test.clone(), mean, std, device=device)\n",
        "\n",
        "part_sim_proc_train = preprocess(part_sim_train.clone(), mean, std, device=device)\n",
        "part_sim_proc_test = preprocess(part_sim_test.clone(), mean, std, device=device)"
      ],
      "metadata": {
        "id": "MXhz2gjfPWNk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 – Building a CFM in PyTorch\n",
        "\n",
        "In this first exercise, we will implement a flexible CFM using PyTorch, which will then serve as a core component the generative unfolding algorithm.\n",
        "\n",
        "The aim is to build a reusable CFM class. This modular design allows us to train and evaluate the CFM multiple times throughout the unfolding process.\n",
        "\n",
        "#### Structure of the CFM\n",
        "\n",
        "Your CFM should be implemented as a Python class with the following components:\n",
        "\n",
        "1. **Initialization (`__init__`)**  \n",
        "   Here, we define:\n",
        "   - `dims_in`: the number of input features,\n",
        "   - `params`: a dictionary of hyperparameters (such as learning rate, network size, and number of training epochs).\n",
        "\n",
        "2. **Building Network (`init_network`)**  \n",
        "  Builds a 3-layer MLP that takes [t, x_t, y] and predicts velocity.\n",
        "\n",
        "3. **Loss Function (`batch_loss`)**  \n",
        "   Implement a method that computes the weighted CFM-loss\n",
        "   - input features `x`,\n",
        "   - condition `y`,\n",
        "   - sample weights `w`.\n",
        "\n",
        "4. **Training Loop (`train_cfm`)**  \n",
        "   Create a training loop that:\n",
        "   - Accepts part-level simulation (`part_sim`) and reco-level simulation (`reco_sim`) samples,\n",
        "   - Optionally takes sample weights,\n",
        "   - Trains the CFM using the Adam optimizer and a learning rate scheduler.\n",
        "\n",
        "5. **Evaluation (`evaluate`)**  \n",
        "   Evaluate the CFM on test reco-level data and return samples from unfolded distribution.\n",
        "\n",
        "\n",
        "Go through the CFM class and fill out the blanks. <br>\n",
        "Hint: To implement the loss function you need to do the following steps:\n",
        "\n",
        "1. Randomly draw the timestep $t \\sim \\mathcal{U}(0,1)$ from a uniform distribution.\n",
        "2. Sample Gaussian noise $\\epsilon \\sim \\mathcal{N}(0,1)$ .\n",
        "3. Compute $x(t, x_0, x_1) = x_0  (1-t) + \\epsilon t $\n",
        "4. Call the network as a function of $x(t, x_0, x_1), y $ and $t$\n",
        "5. Compute the MSE loss between $\\frac{dx(t,x_0, x_1)}{dt}$ and the network output."
      ],
      "metadata": {
        "id": "xnYU2ZYGVUP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFM(nn.Module):\n",
        "    def __init__(self, dims_in: int, params: dict):\n",
        "        \"\"\"\n",
        "        Conditional Flow Matching (CFM) model.\n",
        "\n",
        "        Args:\n",
        "            dims_in (int): Dimensionality of input features.\n",
        "            params (dict): Dictionary containing training and model hyperparameters.\n",
        "                Expected keys:\n",
        "                    - \"hidden_layers\": int, size of hidden layers\n",
        "                    - \"lr\": float, learning rate\n",
        "                    - \"n_epochs\": int, number of training epochs\n",
        "                    - \"batch_size\": int, training batch size\n",
        "                    - \"batch_size_sample\": int, evaluation batch size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dims_in = dims_in\n",
        "        self.params = params\n",
        "        self.init_network()\n",
        "        self.epochs = 0\n",
        "\n",
        "    def init_network(self):\n",
        "        \"\"\"\n",
        "        Initializes a fully connected feedforward neural network.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO:\n",
        "        #    - Build a network to predict the velocity field. What input dimension do you expect?\n",
        "        #    - Use 3 hidden layers of size `self.params[\"hidden_layers\"]`\n",
        "        #    - The last layer need to map back to the physical phase space\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2 * self.dims_in + 1, self.params[\"hidden_layers\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.params[\"hidden_layers\"], self.params[\"hidden_layers\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.params[\"hidden_layers\"], self.dims_in)\n",
        "        )\n",
        "\n",
        "\n",
        "    def batch_loss(self, x_part: torch.Tensor, x_reco: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the batch-wise flow matching loss.\n",
        "\n",
        "        Args:\n",
        "            x_part (Tensor): Particle-level data. Shape: (batch_size, dims_in)\n",
        "            x_reco (Tensor): Reco-level conditioning data. Shape: (batch_size, dims_in)\n",
        "            w (Tensor): Sample weights. Shape: (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Scalar loss value.\n",
        "        \"\"\"\n",
        "        dtype = x_part.dtype\n",
        "        device = x_part.device\n",
        "\n",
        "        # TODO:\n",
        "        # Implement the CFM loss\n",
        "        # Use random interpolation with t ∈ [0, 1]\n",
        "        # Predict velocity with the network and match to target\n",
        "\n",
        "\n",
        "        # Sample interpolation scalar t ∈ [0, 1]\n",
        "        t = torch.rand(size=(x_part.shape[0],1), dtype=dtype, device=device)\n",
        "        # Sample Gaussian noise\n",
        "        noise = torch.randn(size=x_part.shape, dtype=dtype, device=device)\n",
        "        # Interpolate between x_part and noise\n",
        "        xt =\n",
        "        # Predict velocity from interpolated point conditioned on x_reco\n",
        "        model_pred =\n",
        "        # Calculate true velocity (noise - data)\n",
        "        v_target =\n",
        "\n",
        "        return (w.unsqueeze(-1) * (model_pred - v_target) ** 2).mean()\n",
        "\n",
        "    def train_unfolder(self, part, reco, weights=None):\n",
        "        \"\"\"\n",
        "        Trains the CFM model.\n",
        "\n",
        "        Args:\n",
        "            part (Tensor): Particle-level inputs (target). Shape: (N, dims_in)\n",
        "            reco (Tensor): Reco-level inputs (conditioning). Shape: (N, dims_in)\n",
        "            weights (Tensor or None): Optional sample weights. Shape: (N,)\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = torch.ones((part.shape[0]), device=part.device)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(part, reco, weights)\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.params[\"batch_size\"],\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.params[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=len(loader) * self.params[\"n_epochs\"]\n",
        "        )\n",
        "\n",
        "        print(f\"Training CFM for {self.params['n_epochs']} epochs with lr {self.params['lr']}\")\n",
        "        t0 = time.time()\n",
        "\n",
        "        for epoch in range(self.params[\"n_epochs\"]):\n",
        "            losses = []\n",
        "            for x_part, x_reco, weight in loader:\n",
        "                self.net.train()\n",
        "\n",
        "                # TODO: Write training loop\n",
        "\n",
        "                # Reset optimizer\n",
        "\n",
        "                # Calculate batch loss\n",
        "                loss =\n",
        "                # Backpropagation\n",
        "\n",
        "                # Update optimizer\n",
        "\n",
        "                # Update step\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            if epoch % max(1, self.params[\"n_epochs\"] // 5) == 0:\n",
        "                print(\n",
        "                    f\"    Epoch {epoch}: Avg loss = {torch.tensor(losses).mean():.4f}, Time = {round(time.time() - t0, 1)}s\"\n",
        "                )\n",
        "\n",
        "            self.epochs += 1\n",
        "\n",
        "        print(f\"Finished training CFM in {round(time.time() - t0, 1)} seconds\")\n",
        "\n",
        "    def sample(self, x_reco: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Samples particle-level data conditioned on reco-level input.\n",
        "\n",
        "        Args:\n",
        "            x_reco (Tensor): Reco-level conditioning input. Shape: (N, dims_in)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Generated particle-level output. Shape: (N, dims_in)\n",
        "        \"\"\"\n",
        "        dtype = x_reco.dtype\n",
        "        device = x_reco.device\n",
        "\n",
        "        noise = torch.randn(x_reco.size(0), self.dims_in, device=device, dtype=dtype)\n",
        "\n",
        "        def net_wrapper(t, x_t):\n",
        "            # Expand t to batch size and evaluate network at each time step\n",
        "            t_expanded = t * torch.ones_like(x_t[:, [0]], dtype=dtype, device=device)\n",
        "            return self.net(torch.cat([t_expanded, x_t, x_reco], dim=1))\n",
        "\n",
        "        # Integrate velocity field from t=1 to t=0\n",
        "        x_t = odeint(\n",
        "            net_wrapper,\n",
        "            noise,\n",
        "            torch.tensor([1.0, 0.0], dtype=dtype, device=device),\n",
        "            rtol=1e-3, atol=1e-5\n",
        "            )\n",
        "\n",
        "        return x_t[-1]  # Sample at t = 0\n",
        "\n",
        "    def evaluate(self, reco: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Evaluates the model on reco-level input to generate unfolded samples.\n",
        "\n",
        "        Args:\n",
        "            reco (Tensor): Reco-level inputs. Shape: (N, dims_in)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Unfolded particle-level predictions. Shape: (N, dims_in)\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for batch in torch.split(reco, self.params[\"batch_size_sample\"]):\n",
        "                unfold_cfm = self.sample(batch).detach()\n",
        "                predictions.append(unfold_cfm)\n",
        "        return torch.cat(predictions)\n"
      ],
      "metadata": {
        "id": "HU1c0jBuYIvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To iteratively update the prior particle-level distribution, we will also need a classifier. As we have already implemted one in the first tutorial, we can just recycle the `Classifier` class."
      ],
      "metadata": {
        "id": "ue7jRcvD2lOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, dims_in, params):\n",
        "        \"\"\"\n",
        "        Initializes the classifier model.\n",
        "\n",
        "        Args:\n",
        "            dims_in (int): Dimensionality of input features.\n",
        "            params (dict): Dictionary of model and training hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dims_in = dims_in\n",
        "        self.params = params\n",
        "        self.init_network()\n",
        "\n",
        "    def init_network(self):\n",
        "        \"\"\"\n",
        "        Initializes a fully connected feedforward neural network.\n",
        "        \"\"\"\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.dims_in, self.params[\"hidden_layers\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.params[\"hidden_layers\"], self.params[\"hidden_layers\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.params[\"hidden_layers\"], self.params[\"hidden_layers\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.params[\"hidden_layers\"], 1),\n",
        "        )\n",
        "\n",
        "    def batch_loss(self, x, y, w):\n",
        "        \"\"\"\n",
        "        Computes the weighted BCE loss for a batch.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input features.\n",
        "            y (Tensor): Labels (0 or 1).\n",
        "            w (Tensor): Sample weights.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Scalar loss value.\n",
        "        \"\"\"\n",
        "        pred = self.net(x).squeeze()\n",
        "        loss = torch.nn.BCEWithLogitsLoss(weight=w)(pred, y)\n",
        "        return loss\n",
        "\n",
        "    def train_classifier(self,\n",
        "                         data_true,\n",
        "                         data_fake,\n",
        "                         weights_true=None,\n",
        "                         weights_fake=None):\n",
        "        \"\"\"\n",
        "        Trains the classifier on provided true (y=1) and fake (y=0) data.\n",
        "\n",
        "        Args:\n",
        "            data_true (Tensor): Data with label 1.\n",
        "            data_fake (Tensor): Data with label 0.\n",
        "            weights_true (Tensor): Optional weights for true data.\n",
        "            weights_fake (Tensor): Optional weights for fake data.\n",
        "        \"\"\"\n",
        "\n",
        "        device = data_true.device\n",
        "        dtype = data_true.dtype\n",
        "\n",
        "        if weights_true is None:\n",
        "            weights_true = torch.ones((data_true.shape[0]), device=device, dtype=dtype)\n",
        "\n",
        "        if weights_fake is None:\n",
        "            weights_fake = torch.ones((data_fake.shape[0]), device=device, dtype=dtype)\n",
        "\n",
        "        loader_true = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(data_true, weights_true),\n",
        "            batch_size=self.params[\"batch_size\"], shuffle=True,\n",
        "            )\n",
        "\n",
        "        loader_fake = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(data_fake, weights_fake),\n",
        "            batch_size=self.params[\"batch_size\"], shuffle=True,\n",
        "            )\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.params[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer=optimizer,\n",
        "            max_lr=self.params.get(\"max_lr\", 3 * self.params[\"lr\"]),\n",
        "            epochs=self.params[\"n_epochs\"],\n",
        "            steps_per_epoch=min(len(loader_true), len(loader_fake)))\n",
        "\n",
        "        print(f\"Training classifier for {self.params['n_epochs']} epochs with lr {self.params['lr']}\")\n",
        "        t0 = time.time()\n",
        "\n",
        "        for epoch in range(self.params[\"n_epochs\"]):\n",
        "            losses = []\n",
        "            for (x_true, w_true), (x_fake, w_fake) in zip(loader_true, loader_fake):\n",
        "                self.net.train()\n",
        "                label_true = torch.ones((x_true.shape[0]), device=device, dtype=dtype)\n",
        "                label_fake = torch.zeros((x_fake.shape[0]), device=device, dtype=dtype)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.batch_loss(x_true, label_true, w_true)\n",
        "                loss += self.batch_loss(x_fake, label_fake, w_fake)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            if epoch % max(1, self.params[\"n_epochs\"] // 5) == 0:\n",
        "                print(f\"    Epoch {epoch}: Avg loss = {torch.tensor(losses).mean():.4f}, Time = {round(time.time() - t0, 1)}s\")\n",
        "\n",
        "        print(f\"Training finished in {round(time.time() - t0, 1)} seconds.\")\n",
        "\n",
        "    def evaluate(self, data, return_weights=True):\n",
        "        \"\"\"\n",
        "        Evaluates the classifier.\n",
        "\n",
        "        Args:\n",
        "            data (Tensor): Input data.\n",
        "            return_weights (bool): If True, returns likelihood ratios (exp(logits)).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted weights or probabilities.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for batch in torch.split(data, self.params[\"batch_size_sample\"]):\n",
        "                pred = self.net(batch).squeeze().detach()\n",
        "                predictions.append(pred)\n",
        "        predictions = torch.cat(predictions)\n",
        "        return predictions.exp().clip(0, 30) if return_weights else torch.sigmoid(predictions)"
      ],
      "metadata": {
        "id": "nqKGitpe2kgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are going to need a lot of plots showing generated distributions or reweighted distributions we will define a plot function"
      ],
      "metadata": {
        "id": "NJbY8q92jNAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FONTSIZE = 14\n",
        "\n",
        "def plot_generated_distribution(x_part,\n",
        "                                x_reco,\n",
        "                                x_unfolded,\n",
        "                                weights=None,\n",
        "                                bins=50,\n",
        "                                name=None):\n",
        "    \"\"\"\n",
        "    Plots the distribution of a single feature for:\n",
        "      - (weighted) particle-level data,\n",
        "      - (weighted) reco-level data,\n",
        "      - (weighted) unfolded data\n",
        "\n",
        "    Args:\n",
        "        x_part (Tensor): part-level data of shape [N].\n",
        "        x_reco (Tensor): reco-level data of shape [N].\n",
        "        weights (Tensor): Weights of shape [N].\n",
        "        bins (int): Number of bins in the histogram.\n",
        "    \"\"\"\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(x_reco, bins=bins, label=\"reco \", density=True, color=\"royalblue\", histtype=\"step\")\n",
        "    plt.hist(x_unfolded, bins=bins, weights=weights, label=\"unfolded\", density=True, color=\"darkred\", histtype=\"step\")\n",
        "    plt.hist(x_part, bins=bins, label=\"part\", density=True, color=\"black\", histtype=\"step\", weights=weights)\n",
        "\n",
        "    plt.xlabel(r\"${%s}$\" % name,\n",
        "               fontsize=FONTSIZE)\n",
        "\n",
        "    plt.ylabel(\"Density\",\n",
        "              fontsize=FONTSIZE)\n",
        "\n",
        "    plt.legend(frameon=False, fontsize=FONTSIZE)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_reweighted_distribution(true,\n",
        "                                 fake,\n",
        "                                 weights,\n",
        "                                 bins=50,\n",
        "                                name=None):\n",
        "    \"\"\"\n",
        "    Plots the distribution of a single feature for:\n",
        "      - true data,\n",
        "      - fake data (raw),\n",
        "      - fake data reweighted by weights.\n",
        "\n",
        "    Args:\n",
        "        true (Tensor): True data of shape [N, ].\n",
        "        fake (Tensor): Fake data of shape [N, ].\n",
        "        weights (Tensor): Weights for fake data, shape [N].\n",
        "        bins (int): Number of bins in the histogram.\n",
        "    \"\"\"\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(fake, bins=bins, label=\"Sim \", density=True, color=\"royalblue\", histtype=\"step\")\n",
        "    plt.hist(fake, bins=bins, weights=weights, label=\"Sim (reweighted)\", density=True, color=\"darkred\", histtype=\"step\")\n",
        "    plt.hist(true, bins=bins, label=\"Data\", density=True, color=\"black\", histtype=\"step\")\n",
        "\n",
        "    plt.xlabel(r\"${%s}$\" % name,\n",
        "               fontsize=FONTSIZE)\n",
        "\n",
        "    plt.ylabel(\"Density\",\n",
        "              fontsize=FONTSIZE)\n",
        "\n",
        "    plt.legend(frameon=False, fontsize=FONTSIZE)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-HcnECyijCjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cfm parameteres\n",
        "\n",
        "cfm_params = {\"hidden_layers\": 128,\n",
        "              \"lr\": 1.e-3,\n",
        "              \"n_epochs\" : 20,\n",
        "              \"batch_size\" : 8192,\n",
        "              \"batch_size_sample\": 10000}\n",
        "\n",
        "# Useful plotting variables\n",
        "n_features = reco_sim_test.shape[1]\n",
        "\n",
        "binning = [torch.linspace(0, 0.6, 40 + 1),\n",
        "           torch.linspace(-13, -2, 40 + 1),\n",
        "           torch.linspace(0.1, 1.1, 40 + 1)]\n",
        "\n",
        "names = [r\"\\text{Jet width } w\",\n",
        "         r\"\\text{Groomed mass }\\log \\rho\",\n",
        "         r\"\\text{N-subjettiness ratio } \\tau_{21}\"]"
      ],
      "metadata": {
        "id": "pOwvwOvQkL4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 - Single Iteration Generative Unfolding (Unfolder)\n",
        "\n",
        "To get a better understanding of generative unfolding we will start by doing the first iteration explicitly. For one iteration implement the following steps:"
      ],
      "metadata": {
        "id": "HUv0VbCskdeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train a cfm network to learn to sample from the posterior distribution $p(x_{\\text{part}} | x_{\\text{reco}})$"
      ],
      "metadata": {
        "id": "tIPLG3U23Emt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the generative unfolder\n",
        "unfolder =\n",
        "# Train the generative unfolder using part-level simulation and reco-level simulation\n"
      ],
      "metadata": {
        "id": "kFMsclyRkamQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Before we unfold reco-level data we want to ensure that we have closure on simulation. Therefore, start with unfolding the reco-level simulations $x_\\text{reco} \\sim p_\\text{sim}(x_\\text{reco})$."
      ],
      "metadata": {
        "id": "egiurJgw6gND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate reco-level data\n",
        "unfolded_sim_test =\n",
        "\n",
        "# reverse preprocess\n",
        "unfolded_sim_test = preprocess(unfolded_sim_test, mean, std, device=device, reverse=True)"
      ],
      "metadata": {
        "id": "MwDTK6W-345l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Plot reco_sim_test, unfolded_sim_test and part_sim_test"
      ],
      "metadata": {
        "id": "3pYjTNNx8Hno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_features):\n",
        "  plot_generated_distribution(part_sim_test[: , i],\n",
        "                              reco_sim_test[:, i],\n",
        "                              unfolded_sim_test[:, i],\n",
        "                              bins=binning[i],\n",
        "                              name = names[i])"
      ],
      "metadata": {
        "id": "Z2w6cuQq8q2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Unfold $x_\\text{reco} \\sim p_\\text{data}(x_\\text{reco})$"
      ],
      "metadata": {
        "id": "To6ffOaDFdfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate reco-level data\n",
        "unfolded_data_train =\n",
        "unfolded_data_test =\n",
        "\n",
        "# reverse preprocess\n",
        "unfolded_data_test = preprocess(unfolded_data_test, mean, std, device=device, reverse=True)"
      ],
      "metadata": {
        "id": "J1R-d8iqElDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Plot reco_data_test, unfolded_data_test and part_data_test"
      ],
      "metadata": {
        "id": "W5kP98LfGMpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "9E4pbV0LGKuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you observe?"
      ],
      "metadata": {
        "id": "lxUqtEcC5f7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H_mxFKvA5j-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 - Single Iteration Generative Unfolding (Classifier)\n",
        "Implement the following steps:\n",
        "\n",
        "1. To start the first iteration we will train a classifier between $p_\\text{unfold}(x_\\text{part})$ and $p_\\text{sim}(x_\\text{part})$."
      ],
      "metadata": {
        "id": "EWhlPWyG1tMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_params = {\"hidden_layers\": 64,\n",
        "                     \"lr\": 1.e-3,\n",
        "                     \"n_epochs\" : 10,\n",
        "                     \"batch_size\" : 512,\n",
        "                     \"batch_size_sample\": 2000}\n",
        "\n",
        "# Intialize classifier\n",
        "classifier =\n",
        "\n",
        "# Train classifier"
      ],
      "metadata": {
        "id": "zDRqWVFS2bOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluate classifer $w_\\theta (x_\\text{part})$ with $x_\\text{part} \\sim p_\\text{sim}(x_\\text{part})$"
      ],
      "metadata": {
        "id": "YL6DQHOY6k_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate train data\n",
        "w_part_train =\n",
        "\n",
        "# Evaluate test data\n",
        "w_part_test ="
      ],
      "metadata": {
        "id": "WZzb4wy97EfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Check if you classifier learned the likelihood ratio correctly by plotting $p_\\text{unfold}(x_\\text{part})$, $p_\\text{sim}(x_\\text{part})$ and $w_\\theta(x_\\text{part}) p_\\text{sim}(x_\\text{part})$."
      ],
      "metadata": {
        "id": "op-Z3So5471i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "zkYy0iqx40tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 - Iterative Generative Unfolding\n",
        "Now that we have walked through a single iteration of Generative Unfolding step-by-step, it's time to automate the full iterative unfolding procedure.\n",
        "Add all indivual steps to an iterative loop. For later iterations $(i > 0 )$ you can use the pretrained unfolder from the first iteration and reduce the number of epochs to fine-tune the posterior estimation with the updated simulations."
      ],
      "metadata": {
        "id": "TZwhcXP68-f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 3\n",
        "\n",
        "w_part_train = torch.ones((part_sim_train.size(0)), dtype=torch.float, device=device )\n",
        "w_part_test = torch.ones((part_sim_test.size(0)), dtype=torch.float, device=device )\n",
        "for i in range(iterations):\n",
        "    print(f\"Starting with iteration {i}\")\n",
        "    if i == 0:\n",
        "      print(\"Building CFM\")\n",
        "\n",
        "    if i > 0:\n",
        "\n",
        "      print(\"Building classifier\")\n",
        "\n",
        "\n",
        "      print(\"Training classifier\")\n",
        "\n",
        "\n",
        "      print(\"Evaluating classifier\")\n",
        "\n",
        "\n",
        "      print(\"Plotting reweighted simulation.\")\n",
        "\n",
        "\n",
        "      print(f\"Reducing number of epochs of the unfolder.\")\n",
        "\n",
        "\n",
        "    print(\"Training unfolder\")\n",
        "\n",
        "\n",
        "    print(\"Evaluating unfolder on test simulation\")\n",
        "\n",
        "    print(\"Plotting unfolded simulation\")\n",
        "\n",
        "\n",
        "    print(\"Evaluating unfolder on test data\")\n",
        "\n",
        "\n",
        "    print(\"Plotting unfolded data\")\n"
      ],
      "metadata": {
        "id": "rhhA4g3u89O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAle6FUeWsFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}